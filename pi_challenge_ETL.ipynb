{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pi Data Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El problema\n",
    "Un proceso ETL toma datos de un archivo y lo deposita en la tabla dbo.Unificado.\n",
    "Por algún error en estos archivos, aparecieron registros duplicados en la tabla.\n",
    "Consultando con el cliente, nos cuenta que es posible que estas cosas sucedan como consecuencia de errores en el sistema que genera estos archivos, pero que siempre tomemos el ultimo registro que fue copiado, considerando que un registro será duplicado si los campos [ID], [MUESTRA] y [RESULTADO] son iguales en dos filas distintas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consignas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importo librerias\n",
    "\n",
    "De ser necesario, habrá que instalar las mismas si el ambiente no está configurado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "import pyodbc\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  b. Descargo de manera automatica el archivo .csv\n",
    "\n",
    "Aquí lo que hace es utilizar la libreria requests y datetime con el fin de realizar una bajada automatica del .csv, y colocarle la fecha al nombre, ya que el archivo final que vamos a guardar será de caracter incremental, no historico. Esto quiere decir, que sobre el directorio guardado habrá un archivo para 1 vez x semana, necesario para luego correr lo demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo CSV se ha descargado y guardado correctamente como 'nuevas_filas_2023-07-28.csv'.\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from datetime import datetime\n",
    "\n",
    "# Enlace del archivo CSV\n",
    "enlace_csv = \"https://adlssynapsetestfrancis.blob.core.windows.net/challenge/nuevas_filas.csv?sp=r&st=2023-04-20T15:25:12Z&se=2023-12-31T23:25:12Z&spr=https&sv=2021-12-02&sr=b&sig=MZIobvBY6c7ht%2FdFLhtyJ3MZgqa%2B75%2BY3YWntqL%2FStI%3D\"\n",
    "\n",
    "try:\n",
    "    # Realizar la solicitud HTTP para descargar el archivo\n",
    "    respuesta = requests.get(enlace_csv)\n",
    "\n",
    "    # Verificar si la solicitud fue exitosa (código de estado 200)\n",
    "    if respuesta.status_code == 200:\n",
    "        # Obtener el contenido del archivo CSV\n",
    "        contenido_csv = respuesta.content\n",
    "\n",
    "        # Obtener la fecha actual en el formato \"yyyy-mm-dd\"\n",
    "        fecha_actual = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Agregar la fecha al nombre del archivo CSV\n",
    "        nombre_archivo = f\"nuevas_filas_{fecha_actual}.csv\"\n",
    "\n",
    "        # Guardar el contenido en un archivo local\n",
    "        with open(nombre_archivo, 'wb') as archivo_local:\n",
    "            archivo_local.write(contenido_csv)\n",
    "\n",
    "        print(f\"El archivo CSV se ha descargado y guardado correctamente como '{nombre_archivo}'.\")\n",
    "    else:\n",
    "        print(\"No se pudo descargar el archivo CSV. Código de estado:\", respuesta.status_code)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error al descargar el archivo CSV:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Insercion de datos .csv hacia las tablas unificado.\n",
    "\n",
    "* Como lo estoy haciendo de manera local y no montado al ecosistema de Azure, lo que hago es conectarme al servidor, que he configurado para montar el backup (hecho en SQL Server .bak).\n",
    "* Luego lo que va a hacer, es via codigo python, insertar los valores del archivo .csv hacia la base de datos aloja en SQLServer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer la cadena de conexión a la base de datos de SQL Server\n",
    "\n",
    "# Es muy importante aquí tener en cuenta que la configuración depende del usuario en cuestión.\n",
    "\n",
    "server = 'DESKTOP-U84P8HB'\n",
    "database = 'Testing_ETL'\n",
    "username = 'pidata'\n",
    "password = 'admin'\n",
    "conexion_str = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserción de datos completada correctamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Obtener la fecha actual en el formato \"yyyy-mm-dd\"\n",
    "fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Concatenar la fecha al nombre del archivo CSV\n",
    "ruta_archivo_csv = f'nuevas_filas_{fecha_actual}.csv'\n",
    "\n",
    "\n",
    "try:\n",
    "    # Leer el archivo CSV en un DataFrame\n",
    "    df = pd.read_csv(ruta_archivo_csv)\n",
    "\n",
    "    # Agregar la fecha actual en la columna \"FECHA_COPIA\"\n",
    "    fecha_copia_actual = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['FECHA_COPIA'] = fecha_copia_actual\n",
    "\n",
    "    # Conectarse a la base de datos\n",
    "    conexion = pyodbc.connect(conexion_str)\n",
    "\n",
    "    # Crear un cursor para ejecutar las inserciones\n",
    "    cursor = conexion.cursor()\n",
    "\n",
    "    # Ejecutar la instrucción DELETE para eliminar filas con fecha_copia mayor o igual a la fecha actual\n",
    "    delete_query = f'''\n",
    "        DELETE FROM Unificado\n",
    "        WHERE FECHA_COPIA >= '{fecha_actual}'\n",
    "    '''\n",
    "    cursor.execute(delete_query)\n",
    "\n",
    "    # Iterar sobre las filas del DataFrame e insertarlas en la tabla \"Unificado\"\n",
    "    for index, fila in df.iterrows():\n",
    "        query = f'''\n",
    "            INSERT INTO Unificado (CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO, FORMAT, MUESTRA, VALOR, ORIGEN, FECHA_COPIA, RESULTADO)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        '''\n",
    "        parametros = tuple(fila.tolist())\n",
    "        cursor.execute(query, parametros)\n",
    "\n",
    "    # Confirmar los cambios y cerrar el cursor y la conexión\n",
    "    conexion.commit()\n",
    "    cursor.close()\n",
    "    conexion.close()\n",
    "\n",
    "    print(\"Inserción de datos completada correctamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error en la inserción de datos:\", e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Elimino los duplicados.\n",
    "\n",
    "Esto es fundamental, ya que es el problema a abordar por parte del enunciado, debido a que hay que eliminar los duplicados. Vía codigo python, lo hacemos, utilizando también codigo sql para la ejecucion de la misma, creando un CTE, utilizando la función row number... Se podría haber hecho todo el problema, en Azure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados eliminados correctamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    # Conectarse a la base de datos\n",
    "    conexion = pyodbc.connect(conexion_str)\n",
    "\n",
    "    # Crear un cursor para ejecutar la consulta\n",
    "    cursor = conexion.cursor()\n",
    "\n",
    "    # Consulta para seleccionar los registros no duplicados\n",
    "    consulta = '''\n",
    "    WITH t AS (\n",
    "        SELECT *,\n",
    "            RANK() OVER (PARTITION BY ID, MUESTRA, RESULTADO ORDER BY FECHA_COPIA DESC) AS Ranking\n",
    "        FROM [Testing_ETL].[dbo].[Unificado]\n",
    "    )\n",
    "        SELECT CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO, FORMAT, MUESTRA, VALOR, ORIGEN, FECHA_COPIA, RESULTADO\n",
    "        FROM t\n",
    "        WHERE Ranking = 1\n",
    "    '''\n",
    "\n",
    "    # Ejecutar la consulta para obtener los registros no duplicados\n",
    "    cursor.execute(consulta)\n",
    "\n",
    "    # Obtener los resultados de la consulta\n",
    "    registros_no_duplicados = cursor.fetchall()\n",
    "\n",
    "    # Construir la consulta de eliminación\n",
    "    consulta_truncate = '''\n",
    "    TRUNCATE TABLE [Testing_ETL].[dbo].[Unificado]\n",
    "    '''\n",
    "\n",
    "    # Ejecutamos el truncate\n",
    "    cursor.execute(consulta_truncate)\n",
    "    \n",
    "    # Sentencia SQL para insertar los registros no duplicados en la tabla dbo.unificado\n",
    "    insert_query = '''\n",
    "    INSERT INTO [Testing_ETL].[dbo].[unificado] (CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO, FORMAT, MUESTRA, VALOR, ORIGEN, FECHA_COPIA, RESULTADO)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    '''\n",
    "\n",
    "    # Insertar los registros no duplicados en la tabla dbo.unificado\n",
    "    cursor.executemany(insert_query, registros_no_duplicados)\n",
    "\n",
    "\n",
    "    # Confirmar los cambios y cerrar el cursor y la conexión\n",
    "    conexion.commit()\n",
    "    cursor.close()\n",
    "    conexion.close()\n",
    "\n",
    "    print(\"Duplicados eliminados correctamente.\")\n",
    "\n",
    "except pyodbc.Error as e:\n",
    "    print(\"Error en la conexión o ejecución de la consulta:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Dejar de algún modo programado ese proceso para que se ejecute los lunes de cada semana, a las 5:00 AM. (Nivel Teorico)\n",
    "\n",
    "Para crear un flujo de trabajo (pipeline) en Azure Data Factory, sigo los siguientes pasos:\n",
    "\n",
    "1. Ingreso al portal de Azure (https://portal.azure.com) y accedo a mi Azure Data Factory.\n",
    "\n",
    "2. Creo un nuevo flujo de trabajo (pipeline) en ADF.\n",
    "\n",
    "3. Dentro del pipeline, agrego la actividad que deseo ejecutar una vez por semana. Por ejemplo, si tengo una actividad que copia datos desde una fuente a un destino, la agrego al pipeline.\n",
    "\n",
    "4. Luego, creo un disparador (trigger) semanal para programar la ejecución del flujo de trabajo:\n",
    "\n",
    "a. En la parte superior del pipeline, voy a la sección \"Triggers\".\n",
    "\n",
    "b. Creo un nuevo disparador y selecciono \"Schedule\" como tipo de disparador.\n",
    "\n",
    "c. Configuro la programación del disparador para que se ejecute una vez por semana en el día y hora específicos que deseo. Por ejemplo, selecciono \"Weekly\" y elijo el día de la semana y la hora en que quiero que se ejecute el pipeline.\n",
    "\n",
    "5. Asocio el disparador que acabo de crear con el pipeline que contiene la actividad que deseo ejecutar semanalmente.\n",
    "\n",
    "6. Guardo y despliego el flujo de trabajo en Azure Data Factory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
